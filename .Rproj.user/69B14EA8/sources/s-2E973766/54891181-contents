install.packages("AER") 
library(AER) # loading package

getwd()
folder = "C:/Users/fgrev/autored/"
setwd(folder)

ls() # list all variables currently loaded into memory in R
rm(list=ls()) # remove all variables

source("myprog.R") # Run a Program That Include Other Programs
source("myprog.R", echo=TRUE) # Run a Program and display any results (output)
############################################################################################
# class()
my_numeric = 42
my_character = "universe"
my_logical = FALSE 

class(my_numeric) # "numeric"
class(my_character) # "character"
class(my_logical) # "logical"

sapply(data, class) # Get classes of all columns

class(bob$phenotype)

data$x1 <- as.factor(data$x1) # First column is a factor
data$x2 <- as.character(data$x2) # Second column is a character
data$x3 <- as.integer(data$x3)  

data$x1 <- as.numeric(as.character(data$x1))  # Convert one variable to numeric

# factor()
tallas_factor <- factor(tallas) # crear factor de un vector
levels(tallas_factor) # devuelve los niveles (categorias) de la variable
email50_big$number_dropped <- droplevels(email50_big$number) # droplevels()

tab = table(comics$id, comics$align) # contingency table
options(scipen = 999, digits = 3) # Print fewer digits
prop.table(tab) # Joint proportions
prop.table(tab, 2) # Conditional on columns

# Remove align level
comics_filtered <- comics %>%
  filter(align != "Reformed Criminals") %>%
  droplevels()

# print out a value
(avg_read <- mean(df$read)) # 52.23

# Discretize a variable: number to factor
email50_fortified <- email50 %>%
  mutate(num_char_cat = ifelse(num_char < med_num_char, "below median", "at or above median"))

email50_fortified %>% count(num_char_cat) # Count emails in each category

# case_when()
# Create number_yn column in email50
email50_fortified <- email50 %>%
  mutate(
    number_yn = case_when(
      # if number is "none", make number_yn "no"
      number == "none" ~ "no", 
      # if number is not "none", make number_yn "yes"
      number != "none" ~ "yes"  
    )
  )

############################################################################################
# vectors: c()
f = c(7.5,6,5)
f[1] # 7.5
f[2] # 6

my_vector <- 1:10 
a = c(100, 10, 1000)
order(a) # 2 1 3

x = c(1, 2, 3, 4, 5)
class(x) # "numeric"

# cut(): distretize numerical variable
x_discrete = cut(x, breaks = 5)

z = seq(from = 1, to = 5, by = 1)
mean(z)

numeric_vector <- c(1, 10, 49)
character_vector <- c("a", "b", "c")

# Complete the code for boolean_vector
boolean_vector <- c(TRUE, FALSE, TRUE)

# Poker and roulette winnings from Monday to Friday:
poker_vector <- c(140, -50, 20, -120, 240)
roulette_vector <- c(-24, -50, 100, -350, 10)
days_vector <- c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday")
names(poker_vector) <- days_vector
names(roulette_vector) <- days_vector

# Define a new variable based on a selection
poker_wednesday <- poker_vector["Wednesday"]

# Declare variables of different types
my_numeric <- 42
my_character <- "universe"
my_logical <- FALSE 

# Check class of my_numeric
class(my_numeric)

numeric_vector <- c(1, 10, 49)
character_vector <- c("a", "b", "c")

# Complete the code for boolean_vector
boolean_vector <- c(TRUE, FALSE, TRUE)
numeric_vector <- c(1, 10, 49)
character_vector <- c("a", "b", "c")

# Complete the code for boolean_vector
boolean_vector <- c(TRUE, FALSE, TRUE)

# Assign days as names of poker_vector
names(poker_vector) <- c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday")

# Assign days as names of roulette_vectors
names(roulette_vector) <- names(poker_vector)

A_vector <- c(1, 2, 3)
B_vector <- c(4, 5, 6)

# Take the sum of A_vector and B_vector
total_vector <- A_vector + B_vector

# Construct a matrix with 3 rows that contain the numbers 1 up to 9
matrix(1:9, byrow = TRUE, nrow = 3)

# Box office Star Wars (in millions!)
new_hope <- c(460.998, 314.4)
empire_strikes <- c(290.475, 247.900)
return_jedi <- c(309.306, 165.8)

# Create box_office
box_office <- c(new_hope, empire_strikes, return_jedi)

# Construct star_wars_matrix
star_matrix <- matrix(box_office, byrow = TRUE, nrow = 3)

# Poker winnings from Monday to Friday
poker_vector <- c(140, -50, 20, -120, 240)

# Roulette winnings from Monday to Friday
roulette_vector <- c(-24, -50, 100, -350, 10)

# Assign days as names of poker_vector
names(poker_vector) <- c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday")

# Assign days as names of roulette_vectors
names(roulette_vector) <- c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday")

# Take the sum vectors
A_vector <- c(1, 2, 3)
B_vector <- c(4, 5, 6)
total_vector <- A_vector + B_vector

# Print out total_vector
total_vector


# R %in% operator
v1 <- 3
v2 <- 101
t <- c(1,2,3,4,5,6,7,8)
v1 %in% t

############################################################################################
# Matrix()

# Matrix with numerics from 1 up to 9
my_matrix <- matrix(1:9, ncol = 3)

# column bind
data3x2 = cbind(c(1,2,3),c(4,5,6))
data3x2
dim(data3x2) # 3 2
nrow(data3x2)
ncol(data3x2)

# row bind
data2x3 = rbind(c(1,2,3),c(4,5,6))
data2x3
dim(data2x3) # 2 3
nrow(data2x3)
ncol(data2x3)

summary(data2x3)
############################################################################################
# list()
my_vector <- 1:10 # Vector with numerics from 1 up to 10
my_matrix <- matrix(1:9, ncol = 3) # Matrix with numerics from 1 up to 9
my_df <- mtcars[1:10,] # First 10 elements of the built-in data frame mtcars
my_list <- list(my_vector, my_matrix, my_df) # Construct list with these different elements:
names(my_list) <- c("vec", "mat", "df") # Creating a named list
############################################################################################
# Measures of center
x = c(1, 2, 3, 4, 5, 6)
mean(x)
median(x)
var(x)
sd(x)
summary(x)
IQR(x) # interquartile rage

group_by(west_coast) %>%
  summarize(mean(income), median(income))

gap2007 %>%
  group_by(continent) %>%
  summarize(sd(lifeExp),
            IQR(lifeExp),
            n())

############################################################################################
# cor(): correlation

# Compute correlation
ncbirths %>% summarize(N = n(), r = cor(weight, mage))

# Compute correlation for all non-missing pairs
ncbirths %>% summarize(N = n(), r = cor(weight, weeks, use = "pairwise.complete.obs"))

# Compute properties of Anscombe
Anscombe %>%
  group_by(set) %>%
  summarize(
    N = n(), 
    mean_of_x = mean(x), 
    std_dev_of_x = sd(x), 
    mean_of_y = mean(y), 
    std_dev_of_y = sd(y), 
    correlation_between_x_and_y = cor(x,y)
  )

pairs() # generate a grid of scatterplots for all variables in the dataset
############################################################################################
df = read.csv(file="dalbagli.csv", header=TRUE, sep=";", encoding = "UTF-8")
df = tbl_df(df)
df
str(df) # info de las variables en df
nrow(df) # number of rows
############################################################################################
# NA Not a Number
# library(DataCombine) 
# Create data frame
a <- c(1:4, NA)
b <- c(1, NA, 3:5)
ABData <- data.frame(a, b)

# Remove missing values from column a
ASubData <- DropNA(ABData, Var = "a", message = FALSE)

# Remove missing values in columns a and b
ABSubData <- DropNA(ABData, Var = c("a", "b"))

# Remove missing values in all columns of ABDatat
AllSubData <- DropNA(ABData)
############################################################################################
# variable names, column names
library(janitor) # clean variable names
df %>% clean_names() # replace symbols in the varnames to words
ratings <- messy_ratings %>% clean_names("lower_camel") # Reformat to lower camelcase
glimpse(ratings) # Obs, Vars
ratings <- messy_ratings %>% clean_names("snake") # Reformat to snake case (this_is_snake_case)

# select()
Journals %>% select(title, publisher)
Journals %>% select(title:price)
Journals %>% select(title_new = title, publisher) # select 2 variables and rename title to title_new

my_tbl %>% select( new_name_ = starts_with("oldname") ) # find all vars whose names start with oldname, enumerate them, then rename them as new_name_<N>, where N is a number

Journals %>% select(-price) # drop variable price
Journals %>% select(title, starts_with("p")) # keep var title and variables starting with p
Journals %>% select(title, ends_with("e")) # keep var title and variables ending with e
Journals %>% select(contains("pri")) # keep variables that contains pri: price
ratings %>% select(channel, everything()) # Move channel to first column
ratings %>% select(channel, everything(), -ends_with("day")) # Move channel to front and drop all variables endding with "day"

Journals %>% rename(title_new = title) # cambia el nombre de vars mateniendo el order del dataframe, a diferencia de select()
 
# with()
df$c = with(df, ifelse(a==b, a+b, b-a))

############################################################################################
# merge(): add columns

# Inner join: combine columns, keep only obs present in both data tables (default join format for merge())
merge(x = df1, y = df2, by = "varname")
merge(x = df1, y = df2, by.x = "varname_x", by.y = "varname_y")

# Full join: keep all observations
merge(x = df1, y = df2, by = "varname", all = TRUE)

# Left join: keep only observations present on the left table (add info from right table) 
merge(x = df1, y = df2, by = "varname", all.x = TRUE)

# Right join: keep only observations present on the right table (add info from left table) 
merge(x = df1, y = df2, by = "varname", all.y = TRUE)
############################################################################################
# rbind(): add rows
rbind("2015" = df2015, "2016" = df2016, idcol = "year")
#    year profit
# 1: 2015 35000   
# 2: 2015 59000
# 3: 2016 35000
# 4: 2016 86000

rbind(df2015, df2016, idcol = "year")
#    year profit
# 1:    1 35000   
# 2:    1 59000
# 3:    2 35000
# 4:    2 86000

rbind(df2015, df2016, idcol = TRUE)
#     .id profit
# 1:    1 35000   
# 2:    1 59000
# 3:    2 35000
# 4:    2 86000

# Handling missing columns
# if the data tables have different number of columns
rbind("2015" = df2015, "2016" = df2016, idcol = "year", fill=TRUE)
#    year profit revenue
# 1: 2015 35000   NA  
# 2: 2015 59000   NA  
# 3: 2016 35000   1500
# 4: 2016 86000   1000

rbind("2015" = df2015, "2016" = df2016, idcol = "year", fill=TRUE, 
      use.names = TRUE) # matches by their names (no requiere q las columnas tengan el mismo orden en las data tables)

rbind("2015" = df2015, "2016" = df2016, idcol = "year", fill=TRUE, 
      use.names = TRUE) # las junta sin importar que tengan diferente nombre, se guia solo por el orden de als columnas


# rbindlist(): Concatenate rows from a list of data.tables
table_files <- c("sales_2015.csv", "sales_2016.csv")
list_of_tables <- lapply(table_files, fread)
names(list_of_tables) <- c("2015", "2016")
rbindlist(list_of_tables, idcol = "year")




############################################################################################
library(data.table)
tables() # show all data.tables in your R session 
str(df) # structure of data.table df

# Right join
demographics[shipping, on = .(name)] # add info from demographics to shipping

# Left join
shipping[demographics, on = .(name)] # add info from shipping to demographics 

# Inner join
demographics[shipping, on = .(name), nomatch = 0] # keep only rows included in both tables

# Anti join
demographics[!shipping, on = .(name)] # keep rows included demographics and not included in shipping

# define a key variable, so the 'on' argument is not required
setkey(DT, key)
haskey(dt1) # ask if dt1 has a key defined: TRUE/FALSE
key(dt1) # to obtain the key columns you have set (it returns NULL if dt1 has not a defined key)


# DT1[DT2, on][i, j, by]
# |    |   |  |  |
# |    |   |  |   --> grouped by what?
# |    |   |   -----> what to do?
# |    |    --------> on which rows?
# |     ------------> join key columns
#  -----------------> join to which data.table?

# Join then compute
customers <- data.table(name   = c("Mark", "Matt", "Angela", "Michelle"), 
                        gender = c("M", "M", "F", "F"), 
                        age    = c(54, 43, 39, 63))
customers
#       name gender age
#1:     Mark      M  54
#2:     Matt      M  43
#3:   Angela      F  39
#4: Michelle      F  63

purchases <- data.table(name  = c("Mark", "Matt", "Angela", "Michelle"),
                        sales = c(1, 5, 4, 3),
                        spent = c(41.70, 41.78, 50.77, 60.01))
purchases
#       name sales spent
#1:     Mark     1 41.70
#2:     Matt     5 41.78
#3:   Angela     4 50.77
#4: Michelle     3 60.01

customers[purchases, 
          on = .(name)][sales > 1, 
                        j = .(avg_spent = sum(spent) / sum(sales)), 
                        by = .(gender)]
#   gender avg_spent
#1:      M  13.91333
#2:      F  20.00333

# Column creation takes place in the main data.table:
customers[purchases, on = .(name), return_customer := sales > 1]
customers
#       name gender age return_customer
#1:     Mark      M  54           FALSE
#2:     Matt      M  43            TRUE
#3:   Angela      F  39            TRUE
#4: Michelle      F  63            TRUE

customers[shipping, on = .(name), 
          .(avg_age = mean(age)), by = .(gender)]
#   gender  avg_age
#1:      M 46.66667
#2:      F 39.00000

# Join and sum
population[capitals, on = .(city), nomatch = 0,
           j = sum(percentage)]

# How many continents is each country listed in?
continents[life_exp, on = .(country), .N, 
           by = .EACHI]

############################################################################################
library(readr) # (part of tidyverse) to import data in tibble format (tibble~dataframe) 

bakers = read_csv("bakers.csv")
bakers <- read_csv("bakeoff.csv", skip=1) # skip the first line before reading the data
bakers <- read_csv("bakeoff.csv", skip = 1, na = c("", "NA", "UNKNOWN")) # Edit to add list of missing values

glimpse(bakers) # Observations & variables
skim(bakers)

parse_number(varname) # extrae el numero de la variable

############################################################################################
skim(Journals) # Observations & variables
glimpse(Journals)

class(Journals$field) # factor
Journals %>% distinct(field) # distinct(Journals$field)
Journals %>% count(field, sort = TRUE) # count(Journals$field)
Journals %>% count(field == "General") 
Journals %>% group_by(field) %>% summarize(n = n()) # hace lo mismo que count()
Journals %>% count(field, publisher) # count by more tham 1 variable
Journals %>% count(field, publisher) %>% mutate(proportion = n/sum(n)) # add new column with the proportion

bakeoff %>% 
  count(result == "SB")
############################################################################################
# cast
install.packages("AER") # install Rcmdr package
library("AER") # loading package

Journals %>% dplyr::slice(1:4) # get the rows 1:4


# la variable age es del tipo: "36 years". Se tiene que generar una variable con el puro numero
parse_number("36 years") # 36
bakers <- read_csv("bakeoff.csv", col_types = cols(age = col_number())) 

# la variable date es del tipo: "14 August 2012"
parse_date("14 August 2012", format = "%d %B %Y") # 2012-08-14
bakers <- read_csv("bakeoff.csv", col_types = cols(date = col_number(format = "%d %B %Y"))) 

# Type      dplyr::glimpse()    readr::parse_*()      readr::col_*()
# Logical   <lgl>               parse_logical()       col_logical()
# Numerical <int> / <dbl>       parse_number()        col_number()
# Character <chr>               parse_character()     col_character()
# Factor    <fct>               parse_factor(levels)  col_factor(levels)    
# Date      <date>              parse_date(format)    col_date(format)

# Edit code to fix the parsing error 
desserts <- read_csv("desserts.csv",
                     col_types = cols(uk_airdate = col_date(format = "%d %B %Y"), technical = col_number()),
                     na = c("", "NA", "N/A"))


problems(desserts) # View parsing problems

# Cast result a factor (levels = NULL the values are inferred from the unique values in the dataset)
desserts <- read_csv("desserts.csv", 
                     na = c("", "NA", "N/A"),
                     col_types = cols(
                       uk_airdate = col_date(format = "%d %B %Y"),
                       technical = col_number(),                       
                       result = col_factor(levels = NULL)
                     )
)

# Glimpse to view
glimpse(desserts)
############################################################################################
# recode 
Journals %>% mutate(Blackwell_dummy = na_if("publisher", "Blackwell"))


# Edit code to recode "no nut" as missing
desserts_2 <- desserts %>% 
  mutate(nut = recode(nut, "filbert" = "hazelnut", 
                      "no nut" = NA_character_))
# Count rows again 
desserts_2 %>% 
  count(nut, sort = TRUE)

# Edit to recode tech_win as factor
desserts <- desserts %>% 
  mutate(tech_win = recode_factor(technical, `1` = 1,
                                  .default = 0))

# Recode channel as factor: "Channel 4" (0) or not (1)
ratings <- ratings %>% 
  mutate(bbc = recode_factor(channel, 
                             "Channel 4" = 0, 
                             .default = 1))

# Count to compare values                      
desserts %>% 
  count(technical == 1, tech_win)

df %>% replace_na(list(x = 0, y = "unknown")) # en la columna x los NA se reemplazan con 0, y los NA en y se reemplazan con "unknown"

############################################################################################

data("Journals", package = "AER") # get data from package Journals
dim(Journals) # 180  10
names(Journals) # "title" "publisher" "society" "price" "pages" "charpp" "citations" "foundingyear" "subs" "field" 
str(Journals) # 180 obs. of  10 variables
Journals[1, 3] # row 1, column 3
Journals[4,] # entire fourth row
Journals[1:5,"field"] # first 5 values of "field" column

plot(log(subs) ~ log(price/citations), data = Journals) # plot
j_lm <- lm(log(subs) ~ log(price/citations), data = Journals) # OLS linear reggresion
abline(j_lm) # agrega estimacion lineal al grafico
summary(j_lm) # resumen de resultados de regresion lineal

positions <-  order(Journals$price)
Journals[positions,] # Use positions to sort planets_df

##########
data("CPS1985") # get data from package Journals
str(CPS1985) # general info: class of variable
head(CPS1985)
summary(CPS1985) # get some basic info

attach(CPS1985) # con esto se puede referir a las variables de CPS1985 por su nombre y no por CPS1985$education
summary(wage)
mean(wage)
median(wage)
var(wage)
sd(wage)

CPS1985_exp10 = subset(CPS1985, subset = experience > 10)

hist(wage, freq = FALSE)
hist(wage[wage<30], freq = FALSE)
hist(wage[wage<30], freq = FALSE, breaks = c(0,5,10,15,20,25,30))
hist(wage[wage<30], freq = FALSE, breaks = c(0,5,10,15,20,25,30), ylim = c(0,0.1))
hist(log(wage), freq = FALSE)
lines(density(log(wage)), col = 4)

boxplot(wage, horizontal = TRUE)
boxplot(wage)
boxplot(wage~occupation)

summary(occupation) # occupation (categorical variable) absolute frequencies

tab = table(occupation)
prop.table(tab) # occupation (categorical variable) relative frequencies
barplot(tab) # barplot

barplot(tab, horiz = TRUE, main = "Distribution of Occupation")
barplot(tab, horiz = TRUE, las=1, main = "Distribution of Occupation")

pie(tab) # pieplot

#contingency table
xtabs(~ gender + occupation, data = CPS1985)
plot(gender ~ occupation, data = CPS1985) # spine plot

barplot(table(gender,occupation))
barplot(table(gender,occupation), horiz = TRUE)
barplot(table(gender,occupation), horiz = TRUE, las=1)
barplot(table(gender,occupation), horiz = TRUE, las=1, legend=TRUE)
barplot(table(gender,occupation), horiz = TRUE, las=1, legend=TRUE, main = "Occupation by Sex")
barplot(table(gender,occupation), beside = TRUE, las=2, legend=TRUE, main = "Occupation by Sex")

detach(CPS1985)

############################################################################################
# tidyverse
install.packages("tidyverse")
library(gapminder) # database
library(dplyr) # tidyverse
library(ggplot2)

# filter()
gapminder %>%
  filter(year == 2007)

gapminder %>%
  filter(country == "United States")

gapminder %>%
  filter(year == 2007, country == "United States")

common_cyl <- filter(cars, ncyl %in% c(4,6,8)) # Filter cars with 4, 6, 8 cylinders

# arrange(): sort a table base in one avariable
gapminder %>% 
  arrange(gdpPercap) # lowest value first

gapminder %>% 
  arrange(desc(gdpPercap)) # highest value first

# filter() & arrange()
gapminder %>%
  filter(year == 2007) %>%
  arrange(desc(gdpPercap))

# mutate(): change or add variables
gapminder %>% # aqui no se cambia la variable gapminder$pop, este cambio solo se imprime la tabla cambiada
  mutate(pop = pop/1000000)

gapminder %>%
  mutate(gdp = gdpPercap * pop)

gapminder %>% 
  mutate(gdp = gdpPercap * pop) %>%
  filter(year == 2007) %>%
  arrange(desc(gdp))

life = life %>% mutate(west_coast = state %in% c("California", "Oregon", "Washington")) # TRUE in state == "California" or "Oregon" or "Washington"

# summarize()
gapminder %>% 
  summarize(meanLifeExp = mean(lifeExp))

gapminder %>% 
  filter(year == 2007) %>%
  summarize(meanLifeExp = mean(lifeExp))

gapminder %>% 
  filter(year == 2007) %>%
  summarize(meanLifeExp = mean(lifeExp), 
            totalPop = sum(as.numeric(pop)))

# group_by()
by_year = gapminder %>%
            group_by(year) %>%
            summarize(meanLifeExp = mean(lifeExp), 
                      totPop = sum(as.numeric(pop)))

ggplot(by_year, aes(x = year, y = totPop)) + geom_point() + expand_limits(y = 0)


gapminder %>%
  filter(year == 2007) %>%
  group_by(continent) %>%
  summarize(meanLifeExp = mean(lifeExp), 
            totalPop = sum(as.numeric(pop)))

by_year_continent = gapminder %>%
                      group_by(year, continent) %>%
                      summarize(meanLifeExp = mean(lifeExp), 
                      totPop = sum(as.numeric(pop)))

ggplot(by_year_continent, aes(x = year, y = totPop, color = continent)) + geom_point() + expand_limits(y = 0)

# Filter rows where showstopper is UNKNOWN 
bakeoff %>% filter(showstopper == "UNKNOWN") 

# Filter rows where showstopper is NA & show full data
bakeoff %>% filter(is.na(showstopper)) 
glimpse(bakeoff) 

install.packages("skimr")
library(skimr)
skim(bakeoff) # statistics for each variable within the tibble
bakeoff %>% arrange(us_season) %>% glimpse() 

# Edit to filter, group by, and skim
bakeoff %>%  filter(!is.na(us_season)) %>% group_by(us_season)  %>% skim()


# gather(): reshape data
df %>% gather(key = "column_wanted", value = "value_wanted", var_1:var_3)

tidy_ratings <- ratings %>% # Gather and convert episode to factor
  gather(key = "episode", value = "viewers_7day", -series, # remove the column series
         factor_key = TRUE, # store the key values as a factor variable
         na.rm = TRUE) # remove rows that are NA


week_ratings <- ratings2 %>% 
  # Select 7-day viewer ratings
  select(series, ends_with("7day")) %>% 
  # Gather 7-day viewers by episode
  gather(episode, viewers_7day, ends_with("7day"), na.rm = TRUE, factor_key = TRUE)

# Plot 7-day viewers by episode and series
ggplot(week_ratings, aes(x = episode, 
                         y = viewers_7day, 
                         group = series)) +
  geom_line() +
  facet_wrap(~series)


# separate(): separate columns (column_1)
df %>% separate(spice, into = c("spice", "order"),
                convert = TRUE) # convert the new variable into a correct format (number, string, etc)

# unite()



############################################################################################
# ggplot() 
install.packages("tidyverse") 
library(ggplot2)
library(gapminder)
library(dplyr)

##### SCATTER PLOT 
gapminder_2007 = gapminder %>%
  filter(year == 2007) 

ggplot(gapminder_2007, # data 
       aes(x = gdpPercap, y = lifeExp)) + # aestetic
       geom_point() + # type of plot: scatterplot
       ggtitle("Here goes the title") # title

# color
ggplot(gapminder_2007, aes(x = gdpPercap, y = lifeExp, color = continent)) + 
  geom_point() + scale_x_log10()

# size (of the scatterplot points)
ggplot(gapminder_2007, aes(x = gdpPercap, y = lifeExp, color = continent, size = pop)) + 
  geom_point() + scale_x_log10()

# logscales
ggplot(gapminder_2007, aes(x = gdpPercap, y = lifeExp)) +
       geom_point() + scale_x_log10()

ggplot(gapminder_2007, aes(x = pop, y = pop)) +
  geom_point() +  scale_x_log10() +  scale_y_log10()

# Create faceted scatterplot
ggplot(noise, aes(x = x, y = y)) +
  facet_wrap( ~ factor(z)) + 
  geom_point()


# group_by + summarize + ggplot
by_year_continent = gapminder %>%
  group_by(year, continent) %>%
  summarize(meanLifeExp = mean(lifeExp), 
            totPop = sum(as.numeric(pop)))

ggplot(by_year_continent, aes(x = year, y = totPop, color = continent)) + 
  geom_point() + 
  expand_limits(y = 0) # starting y-axis at zero

# Scatterplot with regression line
ggplot(data = bdims, aes(x = hgt, y = wgt)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE)

# Add a line
ggplot(data = bdims, aes(x = hgt, y = wgt)) + 
  geom_point() + 
  add_line(my_slope = 0.5)



##### LINEPLOT ####################
by_year_continent = gapminder %>%
  group_by(year, continent) %>%
  summarize(meanLifeExp = mean(lifeExp), 
            totPop = sum(as.numeric(pop)))

ggplot(by_year_continent, aes(x = year, y = meanLifeExp, color = continent)) + 
  geom_line() + # type of plot: line plpot
  expand_limits(y = 0) # starting y-axis at zero

##### BARPLOT ####################
by_continent = gapminder %>% 
  filter(year == 2007) %>%
  group_by(continent) %>%
  summarize(meanLifeExp = mean(lifeExp))

by_continent

ggplot(by_continent, aes(x = continent, # categorical variable
                         y = meanLifeExp)) + # high of the vars
        geom_col() # barplot

# Plot of gender by align
ggplot(comics, aes(x = align, fill = gender)) +
  geom_bar()

# Plot proportion of gender, conditional on align
ggplot(comics, aes(x = align, fill = gender)) + 
  geom_bar(position = "fill") +
  ylab("proportion")

# Marginal barchart
# Change the order of the levels in align
comics$align <- factor(comics$align, 
                       levels = c("Bad", "Neutral", "Good"))

# Create plot of align
ggplot(comics, aes(x = align)) + 
  geom_bar()

# Conditional barchart
# Plot of alignment broken down by gender
ggplot(comics, aes(x = align)) + 
  geom_bar() +
  facet_wrap(~ gender)

# Create barchart of flavor
ggplot(pies, aes(x = flavor)) + 
  geom_bar(fill = "chartreuse") + 
  theme(axis.text.x = element_text(angle = 90))

# Create plot of proportion of spam by image
email %>%
  mutate(has_image = image > 0) %>%
  ggplot(aes(x = has_image, fill = spam)) +
  geom_bar(position = "fill")

# Reorder levels
email$number_reordered <- factor(email$number, levels = c("none", "small", "big"))

# Construct plot of number_reordered
ggplot(email, aes(x = number_reordered)) +
  geom_bar() +
  facet_wrap(~ spam)

##### HISTOGRAM ####################
gapminder_2007 = gapminder %>%
  filter(year == 2007)

ggplot(gapminder_2007, aes(x = lifeExp)) + geom_histogram()

ggplot(gapminder_2007, aes(x = lifeExp)) + geom_histogram(binwidth = 5)
ggplot(gapminder_2007, aes(x = lifeExp)) + geom_histogram(bins = 50)

cars %>% ggplot(aes(horsepwr)) + geom_histogram() + ggtitle("hist of horsepwr")

cars %>% filter(msrp < 25000) %>% ggplot(aes(horsepwr)) + 
  geom_histogram() + xlim(c(90, 550)) + ggtitle("hist of horsepwr for affordable cars")

cars %>% ggplot(aes(horsepwr)) + geom_histogram(binwidth = 30) + ggtitle("horsepwr with binwidth of 30")

# Create a histogram of population (pop), with x on a log scale
gapminder_1952 <- gapminder %>%
  filter(year == 1952)

ggplot(gapminder_1952, aes(x = pop)) + geom_histogram() + scale_x_log10()

# Faceting is a valuable technique for looking at several conditional distributions at the same time
# Facet hists using hwy mileage and ncyl
common_cyl %>%
  ggplot(aes(x = hwy_mpg)) +
  geom_histogram() +
  facet_grid(ncyl ~ suv) +
  ggtitle("title")


##### BOXPLOT ####################
gapminder_2007 = gapminder %>% filter(year == 2007)
ggplot(gapminder_2007, aes(x = continent, y = lifeExp)) + geom_boxplot()

gapminder_1952 <- gapminder %>% filter(year == 1952)
ggplot(gapminder_1952, aes(x = continent, y = gdpPercap)) + geom_boxplot() + scale_y_log10()

# Create box plots of city mpg by ncyl
ggplot(cars, aes(x = as.factor(ncyl), y = city_mpg)) +
  geom_boxplot()

# Alternative plot: side-by-side box plots
email %>%
  mutate(log_exclaim_mess = log(exclaim_mess + 0.01)) %>%
  ggplot(aes(x = 1, y = log_exclaim_mess)) +
  geom_boxplot() +
  facet_wrap(~ spam)

##### DENSITYPLOT ####################
# Create overlaid density plots for same data
ggplot(common_cyl, aes(x = city_mpg, fill = as.factor(ncyl))) +
  geom_density(alpha = .3)

# Create plot of width
cars %>% 
  ggplot(aes(x = width)) +
  geom_density()

# Generate overlaid density plots
gap2007 %>%
  ggplot(aes(x = lifeExp, fill = continent)) +
  geom_density(alpha = 0.3)

# Create plot for spam and exclaim_mess
email %>%
  mutate(log_exclaim_mess = log(exclaim_mess + 0.01)) %>%
  ggplot(aes(x = log_exclaim_mess)) +
  geom_histogram() +
  facet_wrap(~ spam)

# Alternative plot: Overlaid density plots
email %>%
  mutate(log_exclaim_mess = log(exclaim_mess + .01)) %>%
  ggplot(aes(x = log_exclaim_mess, fill = spam)) +
  geom_density(alpha = 0.3)

############################################################################################
# sample: sampling data
library(openintro)
library(dplyr)
data(county) # Load county data

# Simple random sample of 150 counties
county_srs <- county_noDC %>% sample_n(size = 150)
states_srs %>% group_by(state) %>% count() # Count states by region

# Simple random sample: states_srs
states_srs <- us_regions %>% sample_n(size = 8)
states_srs %>% group_by(region) %>% count() # Count states by region

# Stratified sample
states_str <- us_regions %>% group_by(region) %>% sample_n(size = 2)
states_str %>% group_by(region) %>% count() # Count states by region

############################################################################################
# outliers

# Construct boxplot of msrp
cars %>%
  ggplot(aes(x = 1, y = msrp)) +
  geom_boxplot()

# Exclude outliers from data
cars_no_out <- cars %>%
  filter(msrp < 100000)

# Construct boxplot of msrp using the reduced dataset
cars_no_out %>%
  ggplot(aes(x = 1, y = msrp)) +
  geom_boxplot()


############################################################################################
# lm(): linear model
mod <- lm(formula = wgt ~ hgt, data = bdims)
summary(mod)
class(mod) # "lm"
coef(mod) # coefficients of the model
fitted.values(mod) # y_hat values
residual(mod) # residual values: y - y_hat

lm(formula = poverty ~ hs_grad, data = countyComplete) %>%
  summary()

# Mean of weights equal to mean of fitted values?
mean(fitted.values(mod)) == mean(bdims$wgt) # TRUE

# Mean of the residuals
mean(residuals(mod)) # ~0

library(broom)
bdims_tidy = augment(mod) # attach model estimations to the original data as new variables
.resid # residual
.fitted # y_hat

# fitted values: y_hat
predict(mod) 
augment(mod)$.fitted 

# predict()
new_obs = data.frame(displ = 1.8, year = 2008)
predict(mod, newdata = new_data)

# augment()
new_obs = data.frame(displ = 1.8, year = 2008)
augment(mod, newdata = new_obs)

.hat # Leverage: distance of the observation from the mean of the explanatory variable
# The leverage of an observation in a regression model is defined entirely in terms of the distance of that observation from the mean of the explanatory variable. That is, observations close to the mean of the explanatory variable have low leverage,  while observations far from the mean of the explanatory variable have high leverage. Points of high leverage may or may not be  influential.

# Rank points of high leverage
mod <- lm(HR ~ SB, data = regulars)
mod %>%
  augment() %>%
  arrange(desc(.hat)) %>%
  head()

.cooksd # Cook's distance: measure the influence of observations on the estimation
#As noted previously, observations of high leverage may or may not be influential. The influence of an observation depends not only on its leverage, but also on the magnitude of its residual. Recall that while leverage only takes into account the explanatory variable (x), the residual depends on the response variable (y) and the fitted value (y^).
# Influential points are likely to have high leverage and deviate from the general relationship between the two variables. We measure influence using Cook's distance, which incorporates both the leverage and residual of each observation.

# Rank influential points
mod %>% 
  augment() %>%
  arrange(desc(.cooksd)) %>%
  head()

# Rank high leverage points
mod %>%
  augment() %>%
  arrange(desc(.hat), .cooksd) %>%
  head()


# ordena la data para ver las observaciones con los valores de residuos mas altos 
augment(mod) %>%
  arrange(desc(.resid)) %>%
  head()


textbooks %>%
  mutate(amazNew_cents = amazNew * 100) %>%
  lm(uclaNew ~ amazNew_cents, data = .)

# make a prediction
new_data = data.frame(amazNew = 8.49)
predict(mod, new_data) # 11.11

print(ben)
# wgt   hgt
# 74.8  182.8

# Predict the weight of ben
predict(mod, ben) # 81.01

# Add linear estimated line 
isrs <- broom::augment(mod, newdata = new_data)
ggplot(data = textbooks, aes(x = amazNew, y = uclaNew)) +
  geom_point() + geom_smooth(method = "lm", se = 0)

# Visualize new observations in red
isrs <- broom::augment(mod, newdata = new_data)
ggplot(data = textbooks, aes(x = amazNew, y = uclaNew)) +
  geom_point() + geom_smooth(method = "lm") +
  geom_point(data = isrs, aes(y = .fitted), size = 3, color = "red")

# Adding a regression line to a plot manually
ggplot(data = bdims, aes(x = hgt, y = wgt)) + 
  geom_point() + 
  geom_abline(data = coefs, 
              aes(intercept = `(Intercept)`, slope = hgt),  
              color = "dodgerblue")

# SSE: Sum of the Square Errors
mod_possum <- lm(totalL ~ tailL, data = possum)mod_possum %>%
  augment() %>%
  summarize(SSE = sum(.resid^2))

# RMSE: Root Mean Square Error (Residual Standard Error)
sqrt(sum(residuals(mod)^2) / df.residual(mod))

# degrees of freedom
df.residual(mod)

# Compute the mean of the residuals
mean(residuals(mod))

# null model: y_hat = mean(y)
mod_null <- lm(totalL ~ 1, data = possum)mod_null %>%
  augment(possum) %>%
  summarize(SST = sum(.resid^2))

# View model summary
summary(mod)

# Compute R-squared
bdims_tidy %>%
  summarize(var_y = var(wgt), var_e = var(residuals(mod)) ) %>%
  mutate(R_squared = 1- var_e/var_y)

# Outliers
regulars <- regulars_plus %>%
  filter(!(SB > 60 & HR > 20)) # remove Henderson
coef(lm(HR ~ SB, data = regulars))

nontrivial_players <- mlbBat10 %>% filter(AB >= 10, OBP < 0.5)
mod_cleaner <- lm(SLG ~ OBP, data = nontrivial_players)
summary(mod_cleaner)

ggplot(data = nontrivial_players, aes(x = OBP, y = SLG)) +
  geom_point() + 
  geom_smooth(method = "lm")


# parallel slopes models
mod = lm(totalPr ~ wheels + cond, data = mario_kart)
augmented_mod <- augment(mod) 
glimpse(augmented_mod)
ggplot(augmented_mod, aes(x = wheels, y = totalPr, color = cond)) + geom_point() + # scatter plot
  geom_line(aes(y = .fitted)) # geom_line(): dibuja una linea que une los puntos, como se utiliza color se dibujan 2 lineas


# R2 = 1 - SSE/SST 
# SSE: Sum of Squares Errors sum(.resid^2) = sum((.fitted-y)^2)
# SST: Sum of Squares Total sum((y-mean(y))^2)
# R2_adj = 1 - SSE/SST * (n-1)/(n-p-1)
# n: observations (rows)
# p: number of explanatory variables

# linear model with interaction
lm(y ~ x + z , data = mydata) # y = b0 + b1*x + b2*z + e
lm(y ~ x + z + x:z, data = mydata) # y = b0 + b1*x + b2*z + b3*x*z + e

# Data space is 3D (scatterplot con donde los puntos tienen color = bwt)
data_space <- ggplot(babies, aes(x = gestation, y = age)) +
  geom_point(aes(color = bwt))

# Visualizing interaction models
ggplot(mario_kart, aes(x = duration, y = totalPr, color = cond)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE)

# Simpson's paradox in action
slr <- ggplot(mario_kart, aes(y = totalPr, x = duration)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE)

lm(formula = totalPr ~ duration, data = mario_kart) # model with one slope
slr + aes(color = cond) # plot with two slopes

# 3D visualization
plot_ly(data = babies, z = ~bwt, x = ~gestation, y = ~age, opacity = 0.6) %>%
  add_markers(text = ~case, marker = list(size = 2)) %>%
  add_surface(x = ~x, y = ~y, z = ~plane, showscale = FALSE,
              cmax = 1, surfacecolor = color1, colorscale = col1)

# draw the 3D scatterplot (2 numerical variables as explanatory variables)
p <- plot_ly(data = mario_kart, z = ~totalPr, x = ~duration, y = ~startPr, opacity = 0.6) %>% add_markers() 
p %>% add_surface(x = ~x, y = ~y, z = ~plane, showscale = FALSE) # draw the plane

# Drawing parallel planes in 3D (2 numerical and 1 categorical variables as explanatory variables)
plot_ly(data = babies, z = ~bwt, x = ~gestation, y = ~age, opacity = 0.6) %>%
  add_markers(color = ~factor(smoke), text = ~case, marker = list(size = 2)) %>%
  add_surface(x = ~x, y = ~y, z = ~plane0, showscale = FALSE,
              cmin = 0, cmax = 1, surfacecolor = color1, colorscale = col1) %>%
  add_surface(x = ~x, y = ~y, z = ~plane1, showscale = FALSE,
              cmin = 0, cmax = 1, surfacecolor = color2, colorscale = col1)

# utiliza todo el resto de las variables en el dataset como variables explicativas
lm(bwt ~ . , data = babies)

# utiliza todo el resto de las variables en el dataset menos "case" como variables explicativas
lm(bwt ~ . - case, data = babies)


## Logistic Regression
# A categorical response variable
ggplot(data = heartTr, aes(x = age, y = survived)) +
  geom_jitter(width = 0, height = 0.05, alpha = 0.5)

heartTr <- heartTr %>% mutate(is_alive = ifelse(survived == "alive", 1, 0)) # Making a binary variable

# Visualizing a binary response
data_space <- ggplot(data = heartTr, aes(x = age, y = is_alive)) +
  geom_jitter(width = 0, height = 0.05, alpha = 0.5) # geom_jitter() genera noise a los puntos 

data_space + geom_smooth(method = "lm", se = FALSE) # Regression with a binary response

# Fitting a GLM: Generalized Linear Model 
mod = glm(is_alive ~ age, data = heartTr, family = binomial)
binomial()

augment(mod) # log-odds scale: los .fitted NO son probabilidades
augment(mod, type.predict = "response") # probability scale: los .fitted SI son probabilidades

cheney = data.frame(age = 71, transplant = "treatment") # out-of-sample
augment(mod, newdata = cheney, type.predict = "response") # out-of-sample prediction

# Dibujar lm y glm
data_space +
  geom_smooth(method = "lm", se = FALSE) +
  geom_smooth(method = "glm", se = FALSE, color = "red",
              method.args = list(family = "binomial"))


# binned points and line
data_space <- ggplot(data = MedGPA_binned, aes(x = mean_GPA, y = acceptance_rate)) + geom_point() + geom_line()
MedGPA_plus <- mod %>% augment(type.predict = "response") # augmented model
data_space + geom_line(data = MedGPA_plus, aes(x = GPA, y = .fitted), color = "red") # logistic model on probability scale

# confusion matrix
tidy_mod <- augment(mod, type.predict = "response") %>% mutate(Acceptance_hat = round(.fitted)) # data frame with binary predictions
tidy_mod %>% select(Acceptance, Acceptance_hat) %>% table()

############################################################################################
# blogdown

# 1. Crear un nuevo proyecto website con blogdown

# 2. Crear un sitio web
blogdown::build_site()

# 3. run project
blogdown::serve_site()

# stop project
servr::daemon_stop(1)

############################################################################################
# Inference: the process of making conclutions about a population based on information from a sample
# Suppose a pharmaceutical company is trying to get FDA approval for a new diabetes treatment called drug A. 
# Currently, most doctors prescribe drug B to treat diabetes.
# H0 null hypothesis: Drug A is the same as drug B at treating diabetes.
# H1 alternative hypothesis: Drug A is better than drug B at treating diabetes.




























